{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: systemml\n",
      "Version: 1.3.0\n",
      "Summary: Apache SystemML is a distributed and declarative machine learning platform.\n",
      "Home-page: http://systemml.apache.org/\n",
      "Author: Apache SystemML\n",
      "Author-email: dev@systemml.apache.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\ming\\appdata\\roaming\\python\\python36\\site-packages\n",
      "Requires: scikit-learn, scipy, pandas, numpy, Pillow\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show systemml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package systemml:\n",
      "\n",
      "NAME\n",
      "    systemml\n",
      "\n",
      "DESCRIPTION\n",
      "    # -------------------------------------------------------------\n",
      "    #\n",
      "    # Licensed to the Apache Software Foundation (ASF) under one\n",
      "    # or more contributor license agreements.  See the NOTICE file\n",
      "    # distributed with this work for additional information\n",
      "    # regarding copyright ownership.  The ASF licenses this file\n",
      "    # to you under the Apache License, Version 2.0 (the\n",
      "    # \"License\"); you may not use this file except in compliance\n",
      "    # with the License.  You may obtain a copy of the License at\n",
      "    #\n",
      "    #   http://www.apache.org/licenses/LICENSE-2.0\n",
      "    #\n",
      "    # Unless required by applicable law or agreed to in writing,\n",
      "    # software distributed under the License is distributed on an\n",
      "    # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
      "    # KIND, either express or implied.  See the License for the\n",
      "    # specific language governing permissions and limitations\n",
      "    # under the License.\n",
      "    #\n",
      "    # -------------------------------------------------------------\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    classloader\n",
      "    converters\n",
      "    defmatrix\n",
      "    mlcontext\n",
      "    mllearn (package)\n",
      "    project_info\n",
      "    random (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        systemml.classloader.jvm_stdout\n",
      "        systemml.defmatrix.DMLOp\n",
      "        systemml.defmatrix.matrix\n",
      "        systemml.mlcontext.MLContext\n",
      "        systemml.mlcontext.MLResults\n",
      "        systemml.mlcontext.Matrix\n",
      "        systemml.mlcontext.Script\n",
      "    \n",
      "    class DMLOp(builtins.object)\n",
      "     |  Represents an intermediate node of Abstract syntax tree created to generate the PyDML script\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, inputs, dml=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  MAX_DEPTH = 0\n",
      "    \n",
      "    class MLContext(builtins.object)\n",
      "     |  Wrapper around the new SystemML MLContext.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  sc: SparkContext or SparkSession\n",
      "     |      An instance of pyspark.SparkContext or pyspark.sql.SparkSession.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sc)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  buildTime(self)\n",
      "     |      Display the project build time.\n",
      "     |  \n",
      "     |  close(self)\n",
      "     |      Closes this MLContext instance to cleanup buffer pool, static/local state and scratch space.\n",
      "     |      Note the SparkContext is not explicitly closed to allow external reuse.\n",
      "     |  \n",
      "     |  execute(self, script)\n",
      "     |      Execute a DML / PyDML script.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      script: Script instance\n",
      "     |          Script instance defined with the appropriate input and output variables.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ml_results: MLResults\n",
      "     |          MLResults instance.\n",
      "     |  \n",
      "     |  info(self)\n",
      "     |      Display the project information.\n",
      "     |  \n",
      "     |  isExplain(self)\n",
      "     |      Returns True if program instruction details should be output, False otherwise.\n",
      "     |  \n",
      "     |  isForceGPU(self)\n",
      "     |      Returns True if \"force\" GPU mode is enabled, False otherwise.\n",
      "     |  \n",
      "     |  isGPU(self)\n",
      "     |      Returns True if GPU mode is enabled, False otherwise.\n",
      "     |  \n",
      "     |  isStatistics(self)\n",
      "     |      Returns True if program execution statistics should be output, False otherwise.\n",
      "     |  \n",
      "     |  resetConfig(self)\n",
      "     |      Reset configuration settings to default values.\n",
      "     |  \n",
      "     |  setConfig(self, configFilePath)\n",
      "     |      Set SystemML configuration based on a configuration file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      configFilePath: String\n",
      "     |  \n",
      "     |  setConfigProperty(self, propertyName, propertyValue)\n",
      "     |      Set configuration property, such as setConfigProperty(\"sysml.localtmpdir\", \"/tmp/systemml\").\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      propertyName: String\n",
      "     |      propertyValue: String\n",
      "     |  \n",
      "     |  setExplain(self, explain)\n",
      "     |      Explanation about the program. Mainly intended for developers.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      explain: boolean\n",
      "     |  \n",
      "     |  setExplainLevel(self, explainLevel)\n",
      "     |      Set explain level.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      explainLevel: string\n",
      "     |          Can be one of \"hops\", \"runtime\", \"recompile_hops\", \"recompile_runtime\"\n",
      "     |          or in the above in upper case.\n",
      "     |  \n",
      "     |  setForceGPU(self, enable)\n",
      "     |      Whether or not to force the usage of GPU operators.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      enable: boolean\n",
      "     |  \n",
      "     |  setGPU(self, enable)\n",
      "     |      Whether or not to enable GPU.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      enable: boolean\n",
      "     |  \n",
      "     |  setStatistics(self, statistics)\n",
      "     |      Whether or not to output statistics (such as execution time, elapsed time)\n",
      "     |      about script executions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      statistics: boolean\n",
      "     |  \n",
      "     |  setStatisticsMaxHeavyHitters(self, maxHeavyHitters)\n",
      "     |      The maximum number of heavy hitters that are printed as part of the statistics.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      maxHeavyHitters: int\n",
      "     |  \n",
      "     |  version(self)\n",
      "     |      Display the project version.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MLResults(builtins.object)\n",
      "     |  Wrapper around a Java ML Results object.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  results: JavaObject\n",
      "     |      A Java MLResults object as returned by calling `ml.execute()`.\n",
      "     |  \n",
      "     |  sc: SparkContext\n",
      "     |      SparkContext\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, results, sc)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get(self, *outputs)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      outputs: string, list of strings\n",
      "     |          Output variables as defined inside the DML script.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Matrix(builtins.object)\n",
      "     |  Wrapper around a Java Matrix object.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  javaMatrix: JavaObject\n",
      "     |      A Java Matrix object as returned by calling `ml.execute().get()`.\n",
      "     |  \n",
      "     |  sc: SparkContext\n",
      "     |      SparkContext\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, javaMatrix, sc)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  toDF(self)\n",
      "     |      Convert the Matrix to a PySpark SQL DataFrame.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      PySpark SQL DataFrame\n",
      "     |          A PySpark SQL DataFrame representing the matrix, with\n",
      "     |          one \"__INDEX\" column containing the row index (since Spark\n",
      "     |          DataFrames are unordered), followed by columns of doubles\n",
      "     |          for each column in the matrix.\n",
      "     |  \n",
      "     |  toNumPy(self)\n",
      "     |      Convert the Matrix to a NumPy Array.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      NumPy Array\n",
      "     |          A NumPy Array representing the Matrix object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Script(builtins.object)\n",
      "     |  Instance of a DML/PyDML Script.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  scriptString: string\n",
      "     |      Can be either a file path to a DML script or a DML script itself.\n",
      "     |  \n",
      "     |  scriptType: string\n",
      "     |      Script language, either \"dml\" for DML (R-like) or \"pydml\" for PyDML (Python-like).\n",
      "     |  \n",
      "     |  isResource: boolean\n",
      "     |      If true, scriptString is a path to a resource on the classpath\n",
      "     |  \n",
      "     |  scriptFormat: string\n",
      "     |      Optional script format, either \"auto\" or \"url\" or \"file\" or \"resource\" or \"string\"\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, scriptString, scriptType='dml', isResource=False, scriptFormat='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  clearAll(self)\n",
      "     |      Clear the script string, inputs, outputs, and symbol table.\n",
      "     |  \n",
      "     |  clearIO(self)\n",
      "     |      Clear the inputs and outputs, but not the symbol table.\n",
      "     |  \n",
      "     |  clearIOS(self)\n",
      "     |      Clear the inputs, outputs, and symbol table.\n",
      "     |  \n",
      "     |  clearInputs(self)\n",
      "     |      Clear the inputs.\n",
      "     |  \n",
      "     |  clearOutputs(self)\n",
      "     |      Clear the outputs.\n",
      "     |  \n",
      "     |  clearSymbolTable(self)\n",
      "     |      Clear the symbol table.\n",
      "     |  \n",
      "     |  displayInputParameters(self)\n",
      "     |      Display the script input parameters.\n",
      "     |  \n",
      "     |  displayInputVariables(self)\n",
      "     |      Display the script input variables.\n",
      "     |  \n",
      "     |  displayInputs(self)\n",
      "     |      Display the script inputs.\n",
      "     |  \n",
      "     |  displayOutputVariables(self)\n",
      "     |      Display the script output variables.\n",
      "     |  \n",
      "     |  displayOutputs(self)\n",
      "     |      Display the script outputs.\n",
      "     |  \n",
      "     |  displaySymbolTable(self)\n",
      "     |      Display the script symbol table.\n",
      "     |  \n",
      "     |  getInputVariables(self)\n",
      "     |      Obtain the input variable names.\n",
      "     |  \n",
      "     |  getName(self)\n",
      "     |      Obtain the script name.\n",
      "     |  \n",
      "     |  getOutputVariables(self)\n",
      "     |      Obtain the output variable names.\n",
      "     |  \n",
      "     |  getResults(self)\n",
      "     |      Obtain the results of the script execution.\n",
      "     |  \n",
      "     |  getScriptExecutionString(self)\n",
      "     |      Generate the script execution string, which adds read/load/write/save\n",
      "     |      statements to the beginning and end of the script to execute.\n",
      "     |  \n",
      "     |  getScriptString(self)\n",
      "     |      Obtain the script string (in unicode).\n",
      "     |  \n",
      "     |  getScriptType(self)\n",
      "     |      Obtain the script type.\n",
      "     |  \n",
      "     |  info(self)\n",
      "     |      Display information about the script as a String. This consists of the\n",
      "     |      script type, inputs, outputs, input parameters, input variables, output\n",
      "     |      variables, the symbol table, the script string, and the script execution string.\n",
      "     |  \n",
      "     |  input(self, *args, **kwargs)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      args: name, value tuple\n",
      "     |          where name is a string, and currently supported value formats\n",
      "     |          are double, string, dataframe, rdd, and list of such object.\n",
      "     |      \n",
      "     |      kwargs: dict of name, value pairs\n",
      "     |          To know what formats are supported for name and value, look above.\n",
      "     |  \n",
      "     |  isDML(self)\n",
      "     |      Is the script type DML?\n",
      "     |  \n",
      "     |  isPYDML(self)\n",
      "     |      Is the script type DML?\n",
      "     |  \n",
      "     |  output(self, *names)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      names: string, list of strings\n",
      "     |          Output variables as defined inside the DML script.\n",
      "     |  \n",
      "     |  results(self)\n",
      "     |      Obtain the results of the script execution.\n",
      "     |  \n",
      "     |  setName(self, name)\n",
      "     |      Set the script name.\n",
      "     |  \n",
      "     |  setResults(self, results)\n",
      "     |      Set the results of the script execution.\n",
      "     |  \n",
      "     |  setScriptString(self, scriptString)\n",
      "     |      Set the script string.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      scriptString: string\n",
      "     |          Can be either a file path to a DML script or a DML script itself.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class jvm_stdout(builtins.object)\n",
      "     |  This is useful utility class to get the output of the driver JVM from within a Jupyter notebook\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  parallel_flush: boolean\n",
      "     |      Should flush the stdout in parallel\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __enter__(self)\n",
      "     |  \n",
      "     |  __exit__(self, *args)\n",
      "     |  \n",
      "     |  __init__(self, parallel_flush=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  flush_stdout(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class matrix(builtins.object)\n",
      "     |  matrix class is a python wrapper that implements basic matrix operators, matrix functions\n",
      "     |  as well as converters to common Python types (for example: Numpy arrays, PySpark DataFrame\n",
      "     |  and Pandas DataFrame).\n",
      "     |  \n",
      "     |  The operators supported are:\n",
      "     |  \n",
      "     |  1. Arithmetic operators: +, -, *, /, //, %, ** as well as dot (i.e. matrix multiplication)\n",
      "     |  2. Indexing in the matrix\n",
      "     |  3. Relational/Boolean operators: <, <=, >, >=, ==, !=, &, |\n",
      "     |  \n",
      "     |  In addition, following functions are supported for matrix:\n",
      "     |  \n",
      "     |  1. transpose\n",
      "     |  2. Aggregation functions: sum, mean, var, sd, max, min, argmin, argmax, cumsum\n",
      "     |  3. Global statistical built-In functions: exp, log, abs, sqrt, round, floor, ceil, ceiling, sin, cos, tan, asin, acos, atan, sign, solve\n",
      "     |  \n",
      "     |  For all the above functions, we always return a two dimensional matrix, especially for aggregation functions with axis.\n",
      "     |  For example: Assuming m1 is a matrix of (3, n), NumPy returns a 1d vector of dimension (3,) for operation m1.sum(axis=1)\n",
      "     |  whereas SystemML returns a 2d matrix of dimension (3, 1).\n",
      "     |  \n",
      "     |  Note: an evaluated matrix contains a data field computed by eval method as DataFrame or NumPy array.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import SystemML as sml\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> sml.setSparkContext(sc)\n",
      "     |  \n",
      "     |  Welcome to Apache SystemML!\n",
      "     |  \n",
      "     |  >>> m1 = sml.matrix(np.ones((3,3)) + 2)\n",
      "     |  >>> m2 = sml.matrix(np.ones((3,3)) + 3)\n",
      "     |  >>> m2 = m1 * (m2 + m1)\n",
      "     |  >>> m4 = 1.0 - m2\n",
      "     |  >>> m4\n",
      "     |  # This matrix (mVar5) is backed by below given PyDML script (which is not yet evaluated). To fetch the data of this matrix, invoke toNumPy() or toDF() or toPandas() methods.\n",
      "     |  mVar1 = load(\" \", format=\"csv\")\n",
      "     |  mVar2 = load(\" \", format=\"csv\")\n",
      "     |  mVar3 = mVar2 + mVar1\n",
      "     |  mVar4 = mVar1 * mVar3\n",
      "     |  mVar5 = 1.0 - mVar4\n",
      "     |  save(mVar5, \" \")\n",
      "     |  >>> m2.eval()\n",
      "     |  >>> m2\n",
      "     |  # This matrix (mVar4) is backed by NumPy array. To fetch the NumPy array, invoke toNumPy() method.\n",
      "     |  >>> m4\n",
      "     |  # This matrix (mVar5) is backed by below given PyDML script (which is not yet evaluated). To fetch the data of this matrix, invoke toNumPy() or toDF() or toPandas() methods.\n",
      "     |  mVar4 = load(\" \", format=\"csv\")\n",
      "     |  mVar5 = 1.0 - mVar4\n",
      "     |  save(mVar5, \" \")\n",
      "     |  >>> m4.sum(axis=1).toNumPy()\n",
      "     |  array([[-60.],\n",
      "     |         [-60.],\n",
      "     |         [-60.]])\n",
      "     |  \n",
      "     |  Design Decisions:\n",
      "     |  \n",
      "     |  1. Until eval() method is invoked, we create an AST (not exposed to the user) that consist of unevaluated operations and data required by those operations.\n",
      "     |     As an anology, a spark user can treat eval() method similar to calling RDD.persist() followed by RDD.count().\n",
      "     |  2. The AST consist of two kinds of nodes: either of type matrix or of type DMLOp.\n",
      "     |     Both these classes expose _visit method, that helps in traversing the AST in DFS manner.\n",
      "     |  3. A matrix object can either be evaluated or not.\n",
      "     |     If evaluated, the attribute 'data' is set to one of the supported types (for example: NumPy array or DataFrame). In this case, the attribute 'op' is set to None.\n",
      "     |     If not evaluated, the attribute 'op' which refers to one of the intermediate node of AST and if of type DMLOp.  In this case, the attribute 'data' is set to None.\n",
      "     |  4. DMLOp has an attribute 'inputs' which contains list of matrix objects or DMLOp.\n",
      "     |  5. To simplify the traversal, every matrix object is considered immutable and an matrix operations creates a new matrix object.\n",
      "     |     As an example:\n",
      "     |     `m1 = sml.matrix(np.ones((3,3)))` creates a matrix object backed by 'data=(np.ones((3,3))'.\n",
      "     |     `m1 = m1 * 2` will create a new matrix object which is now backed by 'op=DMLOp( ... )' whose input is earlier created matrix object.\n",
      "     |  6. Left indexing (implemented in __setitem__ method) is a special case, where Python expects the existing object to be mutated.\n",
      "     |     To ensure the above property, we make deep copy of existing object and point any references to the left-indexed matrix to the newly created object.\n",
      "     |     Then the left-indexed matrix is set to be backed by DMLOp consisting of following pydml:\n",
      "     |     left-indexed-matrix = new-deep-copied-matrix\n",
      "     |     left-indexed-matrix[index] = value\n",
      "     |  7. Please use m.print_ast() and/or  type `m` for debugging. Here is a sample session:\n",
      "     |  \n",
      "     |     >>> npm = np.ones((3,3))\n",
      "     |     >>> m1 = sml.matrix(npm + 3)\n",
      "     |     >>> m2 = sml.matrix(npm + 5)\n",
      "     |     >>> m3 = m1 + m2\n",
      "     |     >>> m3\n",
      "     |     mVar2 = load(\" \", format=\"csv\")\n",
      "     |     mVar1 = load(\" \", format=\"csv\")\n",
      "     |     mVar3 = mVar1 + mVar2\n",
      "     |     save(mVar3, \" \")\n",
      "     |     >>> m3.print_ast()\n",
      "     |     - [mVar3] (op).\n",
      "     |       - [mVar1] (data).\n",
      "     |       - [mVar2] (data).\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __and__(self, other)\n",
      "     |      # TODO: Cast the output back into scalar and return boolean results\n",
      "     |  \n",
      "     |  __array__(self, dtype=<class 'numpy.float64'>)\n",
      "     |      As per NumPy from Python,\n",
      "     |      This method is called to obtain an ndarray object when needed. You should always guarantee this returns an actual ndarray object.\n",
      "     |      \n",
      "     |      Using this method, you get back a ndarray object, and subsequent operations on the returned ndarray object will be singlenode.\n",
      "     |  \n",
      "     |  __div__(self, other)\n",
      "     |      Performs division (Python 2 way).\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __floordiv__(self, other)\n",
      "     |  \n",
      "     |  __ge__(self, other)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Implements evaluation of right indexing operations such as m[1,1], m[0:1,], m[:, 0:1]\n",
      "     |  \n",
      "     |  __gt__(self, other)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __init__(self, data, op=None)\n",
      "     |      Constructs a lazy matrix\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data: NumPy ndarray, Pandas DataFrame, scipy sparse matrix or PySpark DataFrame. (data cannot be None for external users, 'data=None' is used internally for lazy evaluation).\n",
      "     |  \n",
      "     |  __le__(self, other)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, other)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __matmul__(self, other)\n",
      "     |      Performs matrix multiplication (infix operator: @). See PEP 465)\n",
      "     |  \n",
      "     |  __mod__(self, other)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __numpy_ufunc__(self, func, method, pos, inputs, **kwargs)\n",
      "     |      This function enables systemml matrix to be compatible with NumPy's ufuncs.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func:  ufunc object that was called.\n",
      "     |      method: string indicating which Ufunc method was called (one of \"__call__\", \"reduce\", \"reduceat\", \"accumulate\", \"outer\", \"inner\").\n",
      "     |      pos: index of self in inputs.\n",
      "     |      inputs:  tuple of the input arguments to the ufunc\n",
      "     |      kwargs: dictionary containing the optional input arguments of the ufunc.\n",
      "     |  \n",
      "     |  __or__(self, other)\n",
      "     |  \n",
      "     |  __pow__(self, other)\n",
      "     |  \n",
      "     |  __radd__(self, other)\n",
      "     |  \n",
      "     |  __rdiv__(self, other)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      This function helps to debug matrix class and also examine the generated PyDML script\n",
      "     |  \n",
      "     |  __rfloordiv__(self, other)\n",
      "     |  \n",
      "     |  __rmod__(self, other)\n",
      "     |  \n",
      "     |  __rmul__(self, other)\n",
      "     |  \n",
      "     |  __rpow__(self, other)\n",
      "     |  \n",
      "     |  __rsub__(self, other)\n",
      "     |  \n",
      "     |  __rtruediv__(self, other)\n",
      "     |      Performs division (Python 3 way).\n",
      "     |  \n",
      "     |  __setitem__(self, index, value)\n",
      "     |      Implements evaluation of left indexing operations such as m[1,1]=2\n",
      "     |  \n",
      "     |  __sub__(self, other)\n",
      "     |  \n",
      "     |  __truediv__(self, other)\n",
      "     |      Performs division (Python 3 way).\n",
      "     |  \n",
      "     |  abs(self)\n",
      "     |  \n",
      "     |  acos(self)\n",
      "     |  \n",
      "     |  arccos(self)\n",
      "     |  \n",
      "     |  arcsin(self)\n",
      "     |  \n",
      "     |  arctan(self)\n",
      "     |  \n",
      "     |  argmax(self, axis=None)\n",
      "     |      Returns the indices of the maximum values along an axis.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      axis : int, optional (only axis=1, i.e. rowIndexMax is supported in this version)\n",
      "     |  \n",
      "     |  argmin(self, axis=None)\n",
      "     |      Returns the indices of the minimum values along an axis.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      axis : int, optional  (only axis=1, i.e. rowIndexMax is supported in this version)\n",
      "     |  \n",
      "     |  asfptype(self)\n",
      "     |  \n",
      "     |  asin(self)\n",
      "     |  \n",
      "     |  astype(self, t)\n",
      "     |  \n",
      "     |  atan(self)\n",
      "     |  \n",
      "     |  ceil(self)\n",
      "     |  \n",
      "     |  ceiling(self)\n",
      "     |  \n",
      "     |  cos(self)\n",
      "     |  \n",
      "     |  cosh(self)\n",
      "     |  \n",
      "     |  cumsum(self, axis=None)\n",
      "     |      Returns the indices of the maximum values along an axis.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      axis : int, optional (only axis=0, i.e. cumsum along the rows is supported in this version)\n",
      "     |  \n",
      "     |  deg2rad(self)\n",
      "     |      Convert angles from degrees to radians.\n",
      "     |  \n",
      "     |  dot(self, other)\n",
      "     |      Numpy way of performing matrix multiplication\n",
      "     |  \n",
      "     |  eval(self)\n",
      "     |      This is a convenience function that calls the global eval method\n",
      "     |  \n",
      "     |  exp(self)\n",
      "     |  \n",
      "     |  exp2(self)\n",
      "     |  \n",
      "     |  expm1(self)\n",
      "     |  \n",
      "     |  floor(self)\n",
      "     |  \n",
      "     |  get_shape(self)\n",
      "     |  \n",
      "     |  hstack(self, other)\n",
      "     |      Stack matrices horizontally (column wise). Invokes cbind internally.\n",
      "     |  \n",
      "     |  ldexp(self, other)\n",
      "     |  \n",
      "     |  log(self, y=None)\n",
      "     |  \n",
      "     |  log10(self)\n",
      "     |  \n",
      "     |  log1p(self)\n",
      "     |  \n",
      "     |  log2(self)\n",
      "     |  \n",
      "     |  logaddexp(self, other)\n",
      "     |  \n",
      "     |  logaddexp2(self, other)\n",
      "     |  \n",
      "     |  logical_not(self)\n",
      "     |  \n",
      "     |  max(self, other=None, axis=None)\n",
      "     |      Compute the maximum value along the specified axis\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other: matrix or numpy array (& other supported types) or scalar\n",
      "     |      axis : int, optional\n",
      "     |  \n",
      "     |  mean(self, axis=None)\n",
      "     |      Compute the arithmetic mean along the specified axis\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      axis : int, optional\n",
      "     |  \n",
      "     |  min(self, other=None, axis=None)\n",
      "     |      Compute the minimum value along the specified axis\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other: matrix or numpy array (& other supported types) or scalar\n",
      "     |      axis : int, optional\n",
      "     |  \n",
      "     |  mod(self, other)\n",
      "     |  \n",
      "     |  moment(self, moment=1, axis=None)\n",
      "     |      Calculates the nth moment about the mean\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      moment : int\n",
      "     |          can be 1, 2, 3 or 4\n",
      "     |      axis : int, optional\n",
      "     |  \n",
      "     |  negative(self)\n",
      "     |  \n",
      "     |  ones_like(self)\n",
      "     |  \n",
      "     |  print_ast(self)\n",
      "     |      Please use m.print_ast() and/or  type `m` for debugging. Here is a sample session:\n",
      "     |      \n",
      "     |      >>> npm = np.ones((3,3))\n",
      "     |      >>> m1 = sml.matrix(npm + 3)\n",
      "     |      >>> m2 = sml.matrix(npm + 5)\n",
      "     |      >>> m3 = m1 + m2\n",
      "     |      >>> m3\n",
      "     |      mVar2 = load(\" \", format=\"csv\")\n",
      "     |      mVar1 = load(\" \", format=\"csv\")\n",
      "     |      mVar3 = mVar1 + mVar2\n",
      "     |      save(mVar3, \" \")\n",
      "     |      >>> m3.print_ast()\n",
      "     |      - [mVar3] (op).\n",
      "     |        - [mVar1] (data).\n",
      "     |        - [mVar2] (data).\n",
      "     |  \n",
      "     |  prod(self)\n",
      "     |      Return the product of all cells in matrix\n",
      "     |  \n",
      "     |  rad2deg(self)\n",
      "     |      Convert angles from radians to degrees.\n",
      "     |  \n",
      "     |  reciprocal(self)\n",
      "     |  \n",
      "     |  remainder(self, other)\n",
      "     |  \n",
      "     |  remove_empty(self, axis=None)\n",
      "     |      Removes all empty rows or columns from the input matrix target X according to specified axis.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      axis : int (0 or 1)\n",
      "     |  \n",
      "     |  replace(self, pattern=None, replacement=None)\n",
      "     |      Removes all empty rows or columns from the input matrix target X according to specified axis.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      pattern : float or int\n",
      "     |      replacement : float or int\n",
      "     |  \n",
      "     |  round(self)\n",
      "     |  \n",
      "     |  save(self, file, format='csv')\n",
      "     |      Allows user to save a matrix to filesystem\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      file: filepath\n",
      "     |      format: can be csv, text or binary or mm\n",
      "     |  \n",
      "     |  sd(self, axis=None)\n",
      "     |      Compute the standard deviation along the specified axis\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      axis : int, optional\n",
      "     |  \n",
      "     |  set_shape(self, shape)\n",
      "     |  \n",
      "     |  sign(self)\n",
      "     |  \n",
      "     |  sin(self)\n",
      "     |  \n",
      "     |  sinh(self)\n",
      "     |  \n",
      "     |  sqrt(self)\n",
      "     |  \n",
      "     |  square(self)\n",
      "     |  \n",
      "     |  sum(self, axis=None)\n",
      "     |      Compute the sum along the specified axis\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      axis : int, optional\n",
      "     |  \n",
      "     |  tan(self)\n",
      "     |  \n",
      "     |  tanh(self)\n",
      "     |  \n",
      "     |  toDF(self)\n",
      "     |      This is a convenience function that calls the global eval method and then converts the matrix object into DataFrame.\n",
      "     |  \n",
      "     |  toNumPy(self)\n",
      "     |      This is a convenience function that calls the global eval method and then converts the matrix object into NumPy array.\n",
      "     |  \n",
      "     |  toPandas(self)\n",
      "     |      This is a convenience function that calls the global eval method and then converts the matrix object into Pandas DataFrame.\n",
      "     |  \n",
      "     |  trace(self)\n",
      "     |      Return the sum of the cells of the main diagonal square matrix\n",
      "     |  \n",
      "     |  transpose(self)\n",
      "     |      Transposes the matrix.\n",
      "     |  \n",
      "     |  var(self, axis=None)\n",
      "     |      Compute the variance along the specified axis.\n",
      "     |      We assume that delta degree of freedom is 1 (unlike NumPy which assumes ddof=0).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      axis : int, optional\n",
      "     |  \n",
      "     |  vstack(self, other)\n",
      "     |      Stack matrices vertically (row wise). Invokes rbind internally.\n",
      "     |  \n",
      "     |  zeros_like(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  shape\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  THROW_ARRAY_CONVERSION_ERROR = False\n",
      "     |  \n",
      "     |  __array_priority__ = 10.2\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  dml = []\n",
      "     |  \n",
      "     |  ml = None\n",
      "     |  \n",
      "     |  ndim = 2\n",
      "     |  \n",
      "     |  script = None\n",
      "     |  \n",
      "     |  systemmlVarID = 0\n",
      "     |  \n",
      "     |  visited = []\n",
      "\n",
      "FUNCTIONS\n",
      "    _java2py(sc, obj)\n",
      "        Convert Java object to Python.\n",
      "    \n",
      "    convertImageToNumPyArr(im, img_shape=None, add_rotated_images=False, add_mirrored_images=False, color_mode='RGB', mean=None)\n",
      "        # Example usage: convertImageToNumPyArr(im, img_shape=(3, 224, 224), add_rotated_images=True, add_mirrored_images=True)\n",
      "        # The above call returns a numpy array of shape (6, 50176) in NCHW format\n",
      "    \n",
      "    convertToLabeledDF(sparkSession, X, y=None)\n",
      "    \n",
      "    convertToMatrixBlock(sc, src, maxSizeBlockInMB=128)\n",
      "    \n",
      "    convertToNumPyArr(sc, mb)\n",
      "    \n",
      "    convertToPandasDF(X)\n",
      "    \n",
      "    convert_caffemodel(sc, deploy_file, caffemodel_file, output_dir, format='binary', is_caffe_installed=False)\n",
      "        Saves the weights and bias in the caffemodel file to output_dir in the specified format.\n",
      "        This method does not requires caffe to be installed.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sc: SparkContext\n",
      "            SparkContext\n",
      "        \n",
      "        deploy_file: string\n",
      "            Path to the input network file\n",
      "        \n",
      "        caffemodel_file: string\n",
      "            Path to the input caffemodel file\n",
      "        \n",
      "        output_dir: string\n",
      "            Path to the output directory\n",
      "        \n",
      "        format: string\n",
      "            Format of the weights and bias (can be binary, csv or text)\n",
      "        \n",
      "        is_caffe_installed: bool\n",
      "            True if caffe is installed\n",
      "    \n",
      "    convert_lmdb_to_jpeg(lmdb_img_file, output_dir)\n",
      "        Saves the images in the lmdb file as jpeg in the output_dir. This method requires caffe to be installed along with lmdb and cv2 package.\n",
      "        To install cv2 package, do `pip install opencv-python`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        lmdb_img_file: string\n",
      "            Path to the input lmdb file\n",
      "        \n",
      "        output_dir: string\n",
      "            Output directory for images (local filesystem)\n",
      "    \n",
      "    createJavaObject(sc, obj_type)\n",
      "        Performs appropriate check if SystemML.jar is available and returns the handle to MLContext object on JVM\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sc: SparkContext\n",
      "            SparkContext\n",
      "        obj_type: Type of object to create ('mlcontext' or 'dummy')\n",
      "    \n",
      "    debug_array_conversion(throwError)\n",
      "    \n",
      "    dml(scriptString)\n",
      "        Create a dml script object based on a string.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        scriptString: string\n",
      "            Can be a path to a dml script or a dml script itself.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        script: Script instance\n",
      "            Instance of a script object.\n",
      "    \n",
      "    dmlFromFile(filePath)\n",
      "        Create a dml script object based on a file path.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        filePath: string\n",
      "            Path to a dml script.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        script: Script instance\n",
      "            Instance of a script object.\n",
      "    \n",
      "    dmlFromResource(resourcePath)\n",
      "        Create a dml script object based on a resource path.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        resourcePath: string\n",
      "            Path to a dml script on the classpath.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        script: Script instance\n",
      "            Instance of a script object.\n",
      "    \n",
      "    dmlFromUrl(url)\n",
      "        Create a dml script object based on a url.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        url: string\n",
      "            URL to a dml script.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        script: Script instance\n",
      "            Instance of a script object.\n",
      "    \n",
      "    eval(outputs, execute=True)\n",
      "        Executes the unevaluated DML script and computes the matrices specified by outputs.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        outputs: list of matrices or a matrix object\n",
      "        execute: specified whether to execute the unevaluated operation or just return the script.\n",
      "    \n",
      "    full(shape, fill_value)\n",
      "        Return a new array of given shape filled with fill_value.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape: tuple of length 2\n",
      "        fill_value: float or int\n",
      "    \n",
      "    getDatasetMean(dataset_name)\n",
      "        Parameters\n",
      "        ----------\n",
      "        dataset_name: Name of the dataset used to train model. This name is artificial name based on dataset used to train the model.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mean: Mean value of model if its defined in the list DATASET_MEAN else None.\n",
      "    \n",
      "    getHopDAG(ml, script, lines=None, conf=None, apply_rewrites=True, with_subgraph=False)\n",
      "        Compile a DML / PyDML script.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        ml: MLContext instance\n",
      "            MLContext instance.\n",
      "        \n",
      "        script: Script instance\n",
      "            Script instance defined with the appropriate input and output variables.\n",
      "        \n",
      "        lines: list of integers\n",
      "            Optional: only display the hops that have begin and end line number equals to the given integers.\n",
      "        \n",
      "        conf: SparkConf instance\n",
      "            Optional spark configuration\n",
      "        \n",
      "        apply_rewrites: boolean\n",
      "            If True, perform static rewrites, perform intra-/inter-procedural analysis to propagate size information into functions and apply dynamic rewrites\n",
      "        \n",
      "        with_subgraph: boolean\n",
      "            If False, the dot graph will be created without subgraphs for statement blocks.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        hopDAG: string\n",
      "            hop DAG in dot format\n",
      "    \n",
      "    getNumCols(numPyArr)\n",
      "    \n",
      "    get_spark_context()\n",
      "        Internal method to get already initialized SparkContext.  Developers should always use\n",
      "        get_spark_context() instead of SparkContext._active_spark_context to ensure SystemML loaded.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sc: SparkContext\n",
      "            SparkContext\n",
      "    \n",
      "    load(file, format='csv')\n",
      "        Allows user to load a matrix from filesystem\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        file: filepath\n",
      "        format: can be csv, text or binary or mm\n",
      "    \n",
      "    pydml(scriptString)\n",
      "        Create a pydml script object based on a string.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        scriptString: string\n",
      "            Can be a path to a pydml script or a pydml script itself.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        script: Script instance\n",
      "            Instance of a script object.\n",
      "    \n",
      "    pydmlFromFile(filePath)\n",
      "        Create a pydml script object based on a file path.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        filePath: string\n",
      "            Path to a pydml script.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        script: Script instance\n",
      "            Instance of a script object.\n",
      "    \n",
      "    pydmlFromResource(resourcePath)\n",
      "        Create a pydml script object based on a resource path.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        resourcePath: string\n",
      "            Path to a pydml script on the classpath.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        script: Script instance\n",
      "            Instance of a script object.\n",
      "    \n",
      "    pydmlFromUrl(url)\n",
      "        Create a pydml script object based on a url.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        url: string\n",
      "            URL to a pydml script.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        script: Script instance\n",
      "            Instance of a script object.\n",
      "    \n",
      "    seq(start=None, stop=None, step=1)\n",
      "        Creates a single column vector with values starting from <start>, to <stop>, in increments of <step>.\n",
      "        Note: Unlike Numpy's arange which returns a row-vector, this returns a column vector.\n",
      "        Also, Unlike Numpy's arange which doesnot include stop, this method includes stop in the interval.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start: int or float [Optional: default = 0]\n",
      "        stop: int or float\n",
      "        step : int float [Optional: default = 1]\n",
      "    \n",
      "    setSparkContext(sc)\n",
      "        Before using the matrix, the user needs to invoke this function if SparkContext is not previously created in the session.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sc: SparkContext\n",
      "            SparkContext\n",
      "    \n",
      "    set_default_jvm_stdout(enable, parallel_flush=True)\n",
      "        This is useful utility method to get the output of the driver JVM from within a Jupyter notebook\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        enable: boolean\n",
      "            Should flush the stdout by default when mlcontext.execute is invoked\n",
      "        \n",
      "        parallel_flush: boolean\n",
      "            Should flush the stdout in parallel\n",
      "    \n",
      "    set_lazy(isLazy)\n",
      "        This method allows users to set whether the matrix operations should be executed in lazy manner.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        isLazy: True if matrix operations should be evaluated in lazy manner.\n",
      "    \n",
      "    solve(A, b)\n",
      "        Computes the least squares solution for system of linear equations A %*% x = b\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import datasets\n",
      "        >>> import SystemML as sml\n",
      "        >>> from pyspark.sql import SparkSession\n",
      "        >>> diabetes = datasets.load_diabetes()\n",
      "        >>> diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "        >>> X_train = diabetes_X[:-20]\n",
      "        >>> X_test = diabetes_X[-20:]\n",
      "        >>> y_train = diabetes.target[:-20]\n",
      "        >>> y_test = diabetes.target[-20:]\n",
      "        >>> sml.setSparkContext(sc)\n",
      "        >>> X = sml.matrix(X_train)\n",
      "        >>> y = sml.matrix(y_train)\n",
      "        >>> A = X.transpose().dot(X)\n",
      "        >>> b = X.transpose().dot(y)\n",
      "        >>> beta = sml.solve(A, b).toNumPy()\n",
      "        >>> y_predicted = X_test.dot(beta)\n",
      "        >>> print('Residual sum of squares: %.2f' % np.mean((y_predicted - y_test) ** 2))\n",
      "        Residual sum of squares: 25282.12\n",
      "\n",
      "DATA\n",
      "    SUPPORTED_TYPES = (<class 'numpy.ndarray'>, <class 'pandas.core.frame....\n",
      "    __all__ = ['MLResults', 'MLContext', 'Script', 'Matrix', 'dml', 'pydml...\n",
      "    default_jvm_stdout = True\n",
      "    default_jvm_stdout_parallel_flush = True\n",
      "\n",
      "FILE\n",
      "    c:\\users\\ming\\appdata\\roaming\\python\\python36\\site-packages\\systemml\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import systemml \n",
    "help(systemml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemml.mllearn import Keras2DML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "sc  = SparkContext(\"local[8]\", \"StreamTest\") # 2 threads, app name \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".master('local')\\\n",
    ".appName('stock')\\\n",
    ".config('spark.excutor.memory','6gb')\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemml import dml, MLContext\n",
    "import numpy as np \n",
    "ml = MLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SystemML Statistics:\n",
      "Total execution time:\t\t0.017 sec.\n",
      "Number of executed Spark inst:\t0.\n",
      "\r\n",
      "\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "# Let's modify the above script to get the Hello World string\n",
    "script = dml(\"\"\" \n",
    "s = 'Hello World' \n",
    "\"\"\").output(\"s\")\n",
    "\n",
    "hello_world_str = ml.execute(script).get(\"s\")\n",
    "\n",
    "print(hello_world_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR Tool version 4.5.3 used for code generation does not match the current runtime version 4.7ANTLR Runtime version 4.5.3 used for parser compilation does not match the current runtime version 4.7ANTLR Tool version 4.5.3 used for code generation does not match the current runtime version 4.7ANTLR Runtime version 4.5.3 used for parser compilation does not match the current runtime version 4.7\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o95.execute.\n: org.apache.sysml.api.mlcontext.MLContextException: Exception when executing script\r\n\tat org.apache.sysml.api.mlcontext.MLContext.execute(MLContext.java:346)\r\n\tat org.apache.sysml.api.mlcontext.MLContext.execute(MLContext.java:319)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.sysml.api.DMLException: java.io.IOException: Failed in creating a non existing dir on HDFS\r\n\tat org.apache.sysml.api.ScriptExecutorUtils.compileRuntimeProgram(ScriptExecutorUtils.java:237)\r\n\tat org.apache.sysml.api.mlcontext.ScriptExecutor.compile(ScriptExecutor.java:195)\r\n\tat org.apache.sysml.api.mlcontext.ScriptExecutor.compile(ScriptExecutor.java:168)\r\n\tat org.apache.sysml.api.mlcontext.ScriptExecutor.execute(ScriptExecutor.java:234)\r\n\tat org.apache.sysml.api.mlcontext.MLContext.execute(MLContext.java:342)\r\n\t... 12 more\r\nCaused by: java.io.IOException: Failed in creating a non existing dir on HDFS\r\n\tat org.apache.sysml.runtime.util.MapReduceTool.createDirIfNotExistOnHDFS(MapReduceTool.java:657)\r\n\tat org.apache.sysml.runtime.util.MapReduceTool.createDirIfNotExistOnHDFS(MapReduceTool.java:637)\r\n\tat org.apache.sysml.api.DMLScript.initHadoopExecution(DMLScript.java:511)\r\n\tat org.apache.sysml.api.ScriptExecutorUtils.compileRuntimeProgram(ScriptExecutorUtils.java:165)\r\n\t... 16 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0777 C:\\Users\\ming\\Documents\\Backup_G\\Advanced_Data_Science_with_IBM\\Applied AI with DeepLearning\\scratch_space\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:491)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:532)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:509)\r\n\tat org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:305)\r\n\tat org.apache.sysml.runtime.util.MapReduceTool.createDirIfNotExistOnHDFS(MapReduceTool.java:653)\r\n\t... 19 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-532381310f10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscript\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# prog = dml(script).input('a',a).input('b',b).ouput('c')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\systemml\\mlcontext.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, script)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdefault_jvm_stdout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mjvm_stdout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparallel_flush\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_jvm_stdout_parallel_flush\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mMLResults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscript_java\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mMLResults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscript_java\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Backup_G\\Spark_software\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Backup_G\\Spark_software\\spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Backup_G\\Spark_software\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o95.execute.\n: org.apache.sysml.api.mlcontext.MLContextException: Exception when executing script\r\n\tat org.apache.sysml.api.mlcontext.MLContext.execute(MLContext.java:346)\r\n\tat org.apache.sysml.api.mlcontext.MLContext.execute(MLContext.java:319)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.sysml.api.DMLException: java.io.IOException: Failed in creating a non existing dir on HDFS\r\n\tat org.apache.sysml.api.ScriptExecutorUtils.compileRuntimeProgram(ScriptExecutorUtils.java:237)\r\n\tat org.apache.sysml.api.mlcontext.ScriptExecutor.compile(ScriptExecutor.java:195)\r\n\tat org.apache.sysml.api.mlcontext.ScriptExecutor.compile(ScriptExecutor.java:168)\r\n\tat org.apache.sysml.api.mlcontext.ScriptExecutor.execute(ScriptExecutor.java:234)\r\n\tat org.apache.sysml.api.mlcontext.MLContext.execute(MLContext.java:342)\r\n\t... 12 more\r\nCaused by: java.io.IOException: Failed in creating a non existing dir on HDFS\r\n\tat org.apache.sysml.runtime.util.MapReduceTool.createDirIfNotExistOnHDFS(MapReduceTool.java:657)\r\n\tat org.apache.sysml.runtime.util.MapReduceTool.createDirIfNotExistOnHDFS(MapReduceTool.java:637)\r\n\tat org.apache.sysml.api.DMLScript.initHadoopExecution(DMLScript.java:511)\r\n\tat org.apache.sysml.api.ScriptExecutorUtils.compileRuntimeProgram(ScriptExecutorUtils.java:165)\r\n\t... 16 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0777 C:\\Users\\ming\\Documents\\Backup_G\\Advanced_Data_Science_with_IBM\\Applied AI with DeepLearning\\scratch_space\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:491)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:532)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:509)\r\n\tat org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:305)\r\n\tat org.apache.sysml.runtime.util.MapReduceTool.createDirIfNotExistOnHDFS(MapReduceTool.java:653)\r\n\t... 19 more\r\n"
     ]
    }
   ],
   "source": [
    "script = \"\"\"\n",
    "c = sum(a %*% t(b))\n",
    "\"\"\"\n",
    "import numpy as np \n",
    "a = np.array([[1,2,3]])\n",
    "b = np.array([[4,5,6]])\n",
    "prog = dml(script).input('a',a).input('b',b)\n",
    "# prog = dml(script).input('a',a).input('b',b).ouput('c')\n",
    "c = ml.execute(prog).get('c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".master('local')\\\n",
    ".appName('stock')\\\n",
    ".config('spark.excutor.memory','6gb')\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.data import mnist_data \n",
    "import numpy as np \n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist_data() \n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(X)\n",
    "X_train = X[:int(.9 * n_samples)] \n",
    "y_train = y[:int(.9 * n_samples)] \n",
    "X_test = X[int(.9 * n_samples):] \n",
    "y_test = y[int(.9 * n_samples):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,663,370\n",
      "Trainable params: 1,663,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout,Flatten\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "input_shape = (1,28,28) if K.image_data_format() == 'channels_first' else (28,28, 1)\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=input_shape, padding='same'))\n",
    "keras_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "keras_model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
    "keras_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "keras_model.add(Flatten())\n",
    "keras_model.add(Dense(512, activation='relu'))\n",
    "keras_model.add(Dropout(0.5))\n",
    "keras_model.add(Dense(10, activation='softmax'))\n",
    "keras_model.summary()\n",
    "keras_model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Scale the input features\n",
    "scale = 0.00390625\n",
    "X_train = X_train*scale\n",
    "X_test = X_test*scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\sequential.py:110: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
      "  warnings.warn('`Sequential.model` is deprecated. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model from weights_dir...\n",
      "SystemML Statistics:\n",
      "Total execution time:\t\t0.000 sec.\n",
      "Number of executed Spark inst:\t0.\n",
      "\n",
      "\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\n",
      "+-------------------+---------------+--------------+------------+---------+-------------------+--------------------+--------------------+\n",
      "|               Name|           Type|        Output|      Weight|     Bias|                Top|              Bottom|Memory* (train/test)|\n",
      "+-------------------+---------------+--------------+------------+---------+-------------------+--------------------+--------------------+\n",
      "|     conv2d_1_input|           Data| (, 1, 28, 28)|            |         |     conv2d_1_input|                    |                 1/0|\n",
      "|           conv2d_1|    Convolution|(, 32, 28, 28)|   [32 X 25]| [32 X 1]|           conv2d_1|      conv2d_1_input|               25/13|\n",
      "|conv2d_1_activation|           ReLU|(, 32, 28, 28)|            |         |conv2d_1_activation|            conv2d_1|               37/25|\n",
      "|    max_pooling2d_1|        Pooling|(, 32, 14, 14)|            |         |    max_pooling2d_1| conv2d_1_activation|               18/15|\n",
      "|           conv2d_2|    Convolution|(, 64, 14, 14)|  [64 X 800]| [64 X 1]|           conv2d_2|     max_pooling2d_1|               41/10|\n",
      "|conv2d_2_activation|           ReLU|(, 64, 14, 14)|            |         |conv2d_2_activation|            conv2d_2|               18/12|\n",
      "|    max_pooling2d_2|        Pooling|  (, 64, 7, 7)|            |         |    max_pooling2d_2| conv2d_2_activation|                 9/8|\n",
      "|          flatten_1|        Flatten|(, 3136, 1, 1)|            |         |          flatten_1|     max_pooling2d_2|                 5/3|\n",
      "|            dense_1|   InnerProduct| (, 512, 1, 1)|[3136 X 512]|[1 X 512]|            dense_1|           flatten_1|              799/14|\n",
      "| dense_1_activation|           ReLU| (, 512, 1, 1)|            |         | dense_1_activation|             dense_1|                 1/1|\n",
      "|          dropout_1|        Dropout| (, 512, 1, 1)|            |         |          dropout_1|  dense_1_activation|                 1/1|\n",
      "|            dense_2|   InnerProduct|  (, 10, 1, 1)|  [512 X 10]| [1 X 10]|            dense_2|           dropout_1|                 3/0|\n",
      "|               loss|SoftmaxWithLoss|  (, 10, 1, 1)|            |         |               loss|dense_2,conv2d_1_...|                 0/0|\n",
      "+-------------------+---------------+--------------+------------+---------+-------------------+--------------------+--------------------+\n",
      "\n",
      "* => memory in megabytes assuming the parameters (input, output activations, weights and backpropagation errors) are in double precision and in dense format.\n",
      "                                                                                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sysml_model = Keras2DML(spark, keras_model, input_shape=(1,28,28), weights='weights_dir')\n",
    "\n",
    "# sysml_model.setConfigProperty(\"sysml.native.blas\", \"auto\")\n",
    "# sysml_model.setGPU(True).setForceGPU(True)\n",
    "sysml_model.summary()\n",
    "# sysml_model.fit(X_train, y_train)\n",
    "# sysml_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras2caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: systemml\n",
      "Version: 1.2.0\n",
      "Summary: Apache SystemML is a distributed and declarative machine learning platform.\n",
      "Home-page: http://systemml.apache.org/\n",
      "Author: Apache SystemML\n",
      "Author-email: dev@systemml.apache.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\user\\appdata\\roaming\\python\\python35\\site-packages\n",
      "Requires: numpy, scipy, scikit-learn, pandas, Pillow\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show systemml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package systemml:\n",
      "\n",
      "NAME\n",
      "    systemml\n",
      "\n",
      "DESCRIPTION\n",
      "    #-------------------------------------------------------------\n",
      "    #\n",
      "    # Licensed to the Apache Software Foundation (ASF) under one\n",
      "    # or more contributor license agreements.  See the NOTICE file\n",
      "    # distributed with this work for additional information\n",
      "    # regarding copyright ownership.  The ASF licenses this file\n",
      "    # to you under the Apache License, Version 2.0 (the\n",
      "    # \"License\"); you may not use this file except in compliance\n",
      "    # with the License.  You may obtain a copy of the License at\n",
      "    #\n",
      "    #   http://www.apache.org/licenses/LICENSE-2.0\n",
      "    #\n",
      "    # Unless required by applicable law or agreed to in writing,\n",
      "    # software distributed under the License is distributed on an\n",
      "    # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
      "    # KIND, either express or implied.  See the License for the\n",
      "    # specific language governing permissions and limitations\n",
      "    # under the License.\n",
      "    #\n",
      "    #-------------------------------------------------------------\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    classloader\n",
      "    converters\n",
      "    defmatrix\n",
      "    mlcontext\n",
      "    mllearn (package)\n",
      "    project_info\n",
      "    random (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        systemml.defmatrix.DMLOp\n",
      "        systemml.defmatrix.matrix\n",
      "        systemml.mlcontext.MLContext\n",
      "        systemml.mlcontext.MLResults\n",
      "        systemml.mlcontext.Matrix\n",
      "        systemml.mlcontext.Script\n",
      "    \n",
      "    class DMLOp(builtins.object)\n",
      "     |  Represents an intermediate node of Abstract syntax tree created to generate the PyDML script\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, inputs, dml=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  MAX_DEPTH = 0\n",
      "    \n",
      "    class MLContext(builtins.object)\n",
      "     |  Wrapper around the new SystemML MLContext.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  sc: SparkContext or SparkSession\n",
      "     |      An instance of pyspark.SparkContext or pyspark.sql.SparkSession.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sc)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  buildTime(self)\n",
      "     |      Display the project build time.\n",
      "     |  \n",
      "     |  close(self)\n",
      "     |      Closes this MLContext instance to cleanup buffer pool, static/local state and scratch space.\n",
      "     |      Note the SparkContext is not explicitly closed to allow external reuse.\n",
      "     |  \n",
      "     |  execute(self, script)\n",
      "     |      Execute a DML / PyDML script.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      script: Script instance\n",
      "     |          Script instance defined with the appropriate input and output variables.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ml_results: MLResults\n",
      "     |          MLResults instance.\n",
      "     |  \n",
      "     |  info(self)\n",
      "     |      Display the project information.\n",
      "     |  \n",
      "     |  isExplain(self)\n",
      "     |      Returns True if program instruction details should be output, False otherwise.\n",
      "     |  \n",
      "     |  isForceGPU(self)\n",
      "     |      Returns True if \"force\" GPU mode is enabled, False otherwise.\n",
      "     |  \n",
      "     |  isGPU(self)\n",
      "     |      Returns True if GPU mode is enabled, False otherwise.\n",
      "     |  \n",
      "     |  isStatistics(self)\n",
      "     |      Returns True if program execution statistics should be output, False otherwise.\n",
      "     |  \n",
      "     |  resetConfig(self)\n",
      "     |      Reset configuration settings to default values.\n",
      "     |  \n",
      "     |  setConfig(self, configFilePath)\n",
      "     |      Set SystemML configuration based on a configuration file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      configFilePath: String\n",
      "     |  \n",
      "     |  setConfigProperty(self, propertyName, propertyValue)\n",
      "     |      Set configuration property, such as setConfigProperty(\"sysml.localtmpdir\", \"/tmp/systemml\").\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      propertyName: String\n",
      "     |      propertyValue: String\n",
      "     |  \n",
      "     |  setExplain(self, explain)\n",
      "     |      Explanation about the program. Mainly intended for developers.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      explain: boolean\n",
      "     |  \n",
      "     |  setExplainLevel(self, explainLevel)\n",
      "     |      Set explain level.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      explainLevel: string\n",
      "     |          Can be one of \"hops\", \"runtime\", \"recompile_hops\", \"recompile_runtime\"\n",
      "     |          or in the above in upper case.\n",
      "     |  \n",
      "     |  setForceGPU(self, enable)\n",
      "     |      Whether or not to force the usage of GPU operators.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      enable: boolean\n",
      "     |  \n",
      "     |  setGPU(self, enable)\n",
      "     |      Whether or not to enable GPU.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      enable: boolean\n",
      "     |  \n",
      "     |  setStatistics(self, statistics)\n",
      "     |      Whether or not to output statistics (such as execution time, elapsed time)\n",
      "     |      about script executions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      statistics: boolean\n",
      "     |  \n",
      "     |  setStatisticsMaxHeavyHitters(self, maxHeavyHitters)\n",
      "     |      The maximum number of heavy hitters that are printed as part of the statistics.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      maxHeavyHitters: int\n",
      "     |  \n",
      "     |  version(self)\n",
      "     |      Display the project version.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MLResults(builtins.object)\n",
      "     |  Wrapper around a Java ML Results object.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  results: JavaObject\n",
      "     |      A Java MLResults object as returned by calling `ml.execute()`.\n",
      "     |  \n",
      "     |  sc: SparkContext\n",
      "     |      SparkContext\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, results, sc)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get(self, *outputs)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      outputs: string, list of strings\n",
      "     |          Output variables as defined inside the DML script.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Matrix(builtins.object)\n",
      "     |  Wrapper around a Java Matrix object.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  javaMatrix: JavaObject\n",
      "     |      A Java Matrix object as returned by calling `ml.execute().get()`.\n",
      "     |  \n",
      "     |  sc: SparkContext\n",
      "     |      SparkContext\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, javaMatrix, sc)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  toDF(self)\n",
      "     |      Convert the Matrix to a PySpark SQL DataFrame.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      PySpark SQL DataFrame\n",
      "     |          A PySpark SQL DataFrame representing the matrix, with\n",
      "     |          one \"__INDEX\" column containing the row index (since Spark\n",
      "     |          DataFrames are unordered), followed by columns of doubles\n",
      "     |          for each column in the matrix.\n",
      "     |  \n",
      "     |  toNumPy(self)\n",
      "     |      Convert the Matrix to a NumPy Array.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      NumPy Array\n",
      "     |          A NumPy Array representing the Matrix object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Script(builtins.object)\n",
      "     |  Instance of a DML/PyDML Script.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  scriptString: string\n",
      "     |      Can be either a file path to a DML script or a DML script itself.\n",
      "     |  \n",
      "     |  scriptType: string\n",
      "     |      Script language, either \"dml\" for DML (R-like) or \"pydml\" for PyDML (Python-like).\n",
      "     |  \n",
      "     |  isResource: boolean\n",
      "     |      If true, scriptString is a path to a resource on the classpath\n",
      "     |  \n",
      "     |  scriptFormat: string\n",
      "     |      Optional script format, either \"auto\" or \"url\" or \"file\" or \"resource\" or \"string\"\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, scriptString, scriptType='dml', isResource=False, scriptFormat='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  clearAll(self)\n",
      "     |      Clear the script string, inputs, outputs, and symbol table.\n",
      "     |  \n",
      "     |  clearIO(self)\n",
      "     |      Clear the inputs and outputs, but not the symbol table.\n",
      "     |  \n",
      "     |  clearIOS(self)\n",
      "     |      Clear the inputs, outputs, and symbol table.\n",
      "     |  \n",
      "     |  clearInputs(self)\n",
      "     |      Clear the inputs.\n",
      "     |  \n",
      "     |  clearOutputs(self)\n",
      "     |      Clear the outputs.\n",
      "     |  \n",
      "     |  clearSymbolTable(self)\n",
      "     |      Clear the symbol table.\n",
      "     |  \n",
      "     |  displayInputParameters(self)\n",
      "     |      Display the script input parameters.\n",
      "     |  \n",
      "     |  displayInputVariables(self)\n",
      "     |      Display the script input variables.\n",
      "     |  \n",
      "     |  displayInputs(self)\n",
      "     |      Display the script inputs.\n",
      "     |  \n",
      "     |  displayOutputVariables(self)\n",
      "     |      Display the script output variables.\n",
      "     |  \n",
      "     |  displayOutputs(self)\n",
      "     |      Display the script outputs.\n",
      "     |  \n",
      "     |  displaySymbolTable(self)\n",
      "     |      Display the script symbol table.\n",
      "     |  \n",
      "     |  getInputVariables(self)\n",
      "     |      Obtain the input variable names.\n",
      "     |  \n",
      "     |  getName(self)\n",
      "     |      Obtain the script name.\n",
      "     |  \n",
      "     |  getOutputVariables(self)\n",
      "     |      Obtain the output variable names.\n",
      "     |  \n",
      "     |  getResults(self)\n",
      "     |      Obtain the results of the script execution.\n",
      "     |  \n",
      "     |  getScriptExecutionString(self)\n",
      "     |      Generate the script execution string, which adds read/load/write/save\n",
      "     |      statements to the beginning and end of the script to execute.\n",
      "     |  \n",
      "     |  getScriptString(self)\n",
      "     |      Obtain the script string (in unicode).\n",
      "     |  \n",
      "     |  getScriptType(self)\n",
      "     |      Obtain the script type.\n",
      "     |  \n",
      "     |  info(self)\n",
      "     |      Display information about the script as a String. This consists of the\n",
      "     |      script type, inputs, outputs, input parameters, input variables, output\n",
      "     |      variables, the symbol table, the script string, and the script execution string.\n",
      "     |  \n",
      "     |  input(self, *args, **kwargs)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      args: name, value tuple\n",
      "     |          where name is a string, and currently supported value formats\n",
      "     |          are double, string, dataframe, rdd, and list of such object.\n",
      "     |      \n",
      "     |      kwargs: dict of name, value pairs\n",
      "     |          To know what formats are supported for name and value, look above.\n",
      "     |  \n",
      "     |  isDML(self)\n",
      "     |      Is the script type DML?\n",
      "     |  \n",
      "     |  isPYDML(self)\n",
      "     |      Is the script type DML?\n",
      "     |  \n",
      "     |  output(self, *names)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      names: string, list of strings\n",
      "     |          Output variables as defined inside the DML script.\n",
      "     |  \n",
      "     |  results(self)\n",
      "     |      Obtain the results of the script execution.\n",
      "     |  \n",
      "     |  setName(self, name)\n",
      "     |      Set the script name.\n",
      "     |  \n",
      "     |  setResults(self, results)\n",
      "     |      Set the results of the script execution.\n",
      "     |  \n",
      "     |  setScriptString(self, scriptString)\n",
      "     |      Set the script string.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      scriptString: string\n",
      "     |          Can be either a file path to a DML script or a DML script itself.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class matrix(builtins.object)\n",
      "     |  matrix class is a python wrapper that implements basic matrix operators, matrix functions\n",
      "     |  as well as converters to common Python types (for example: Numpy arrays, PySpark DataFrame\n",
      "     |  and Pandas DataFrame). \n",
      "     |  \n",
      "     |  The operators supported are:\n",
      "     |  \n",
      "     |  1. Arithmetic operators: +, -, *, /, //, %, ** as well as dot (i.e. matrix multiplication)\n",
      "     |  2. Indexing in the matrix\n",
      "     |  3. Relational/Boolean operators: <, <=, >, >=, ==, !=, &, |\n",
      "     |  \n",
      "     |  In addition, following functions are supported for matrix:\n",
      "     |  \n",
      "     |  1. transpose\n",
      "     |  2. Aggregation functions: sum, mean, var, sd, max, min, argmin, argmax, cumsum\n",
      "     |  3. Global statistical built-In functions: exp, log, abs, sqrt, round, floor, ceil, ceiling, sin, cos, tan, asin, acos, atan, sign, solve\n",
      "     |  \n",
      "     |  For all the above functions, we always return a two dimensional matrix, especially for aggregation functions with axis. \n",
      "     |  For example: Assuming m1 is a matrix of (3, n), NumPy returns a 1d vector of dimension (3,) for operation m1.sum(axis=1)\n",
      "     |  whereas SystemML returns a 2d matrix of dimension (3, 1).\n",
      "     |  \n",
      "     |  Note: an evaluated matrix contains a data field computed by eval method as DataFrame or NumPy array.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import SystemML as sml\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> sml.setSparkContext(sc)\n",
      "     |  \n",
      "     |  Welcome to Apache SystemML!\n",
      "     |  \n",
      "     |  >>> m1 = sml.matrix(np.ones((3,3)) + 2)\n",
      "     |  >>> m2 = sml.matrix(np.ones((3,3)) + 3)\n",
      "     |  >>> m2 = m1 * (m2 + m1)\n",
      "     |  >>> m4 = 1.0 - m2\n",
      "     |  >>> m4\n",
      "     |  # This matrix (mVar5) is backed by below given PyDML script (which is not yet evaluated). To fetch the data of this matrix, invoke toNumPy() or toDF() or toPandas() methods.\n",
      "     |  mVar1 = load(\" \", format=\"csv\")\n",
      "     |  mVar2 = load(\" \", format=\"csv\")\n",
      "     |  mVar3 = mVar2 + mVar1\n",
      "     |  mVar4 = mVar1 * mVar3\n",
      "     |  mVar5 = 1.0 - mVar4\n",
      "     |  save(mVar5, \" \")\n",
      "     |  >>> m2.eval()\n",
      "     |  >>> m2\n",
      "     |  # This matrix (mVar4) is backed by NumPy array. To fetch the NumPy array, invoke toNumPy() method.\n",
      "     |  >>> m4\n",
      "     |  # This matrix (mVar5) is backed by below given PyDML script (which is not yet evaluated). To fetch the data of this matrix, invoke toNumPy() or toDF() or toPandas() methods.\n",
      "     |  mVar4 = load(\" \", format=\"csv\")\n",
      "     |  mVar5 = 1.0 - mVar4\n",
      "     |  save(mVar5, \" \")\n",
      "     |  >>> m4.sum(axis=1).toNumPy()\n",
      "     |  array([[-60.],\n",
      "     |         [-60.],\n",
      "     |         [-60.]])\n",
      "     |  \n",
      "     |  Design Decisions:\n",
      "     |  \n",
      "     |  1. Until eval() method is invoked, we create an AST (not exposed to the user) that consist of unevaluated operations and data required by those operations.\n",
      "     |     As an anology, a spark user can treat eval() method similar to calling RDD.persist() followed by RDD.count().\n",
      "     |  2. The AST consist of two kinds of nodes: either of type matrix or of type DMLOp.\n",
      "     |     Both these classes expose _visit method, that helps in traversing the AST in DFS manner.\n",
      "     |  3. A matrix object can either be evaluated or not.\n",
      "     |     If evaluated, the attribute 'data' is set to one of the supported types (for example: NumPy array or DataFrame). In this case, the attribute 'op' is set to None.\n",
      "     |     If not evaluated, the attribute 'op' which refers to one of the intermediate node of AST and if of type DMLOp.  In this case, the attribute 'data' is set to None.\n",
      "     |  4. DMLOp has an attribute 'inputs' which contains list of matrix objects or DMLOp.\n",
      "     |  5. To simplify the traversal, every matrix object is considered immutable and an matrix operations creates a new matrix object.\n",
      "     |     As an example: \n",
      "     |     `m1 = sml.matrix(np.ones((3,3)))` creates a matrix object backed by 'data=(np.ones((3,3))'.\n",
      "     |     `m1 = m1 * 2` will create a new matrix object which is now backed by 'op=DMLOp( ... )' whose input is earlier created matrix object.\n",
      "     |  6. Left indexing (implemented in __setitem__ method) is a special case, where Python expects the existing object to be mutated.\n",
      "     |     To ensure the above property, we make deep copy of existing object and point any references to the left-indexed matrix to the newly created object.\n",
      "     |     Then the left-indexed matrix is set to be backed by DMLOp consisting of following pydml:\n",
      "     |     left-indexed-matrix = new-deep-copied-matrix\n",
      "     |     left-indexed-matrix[index] = value\n",
      "     |  7. Please use m.print_ast() and/or  type `m` for debugging. Here is a sample session:\n",
      "     |  \n",
      "     |     >>> npm = np.ones((3,3))\n",
      "     |     >>> m1 = sml.matrix(npm + 3)\n",
      "     |     >>> m2 = sml.matrix(npm + 5)\n",
      "     |     >>> m3 = m1 + m2\n",
      "     |     >>> m3\n",
      "     |     mVar2 = load(\" \", format=\"csv\")\n",
      "     |     mVar1 = load(\" \", format=\"csv\")\n",
      "     |     mVar3 = mVar1 + mVar2\n",
      "     |     save(mVar3, \" \")\n",
      "     |     >>> m3.print_ast()\n",
      "     |     - [mVar3] (op).\n",
      "     |       - [mVar1] (data).\n",
      "     |       - [mVar2] (data).\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __and__(self, other)\n",
      "     |      # TODO: Cast the output back into scalar and return boolean results\n",
      "     |  \n",
      "     |  __array__(self, dtype=<class 'numpy.float64'>)\n",
      "     |      As per NumPy from Python,\n",
      "     |      This method is called to obtain an ndarray object when needed. You should always guarantee this returns an actual ndarray object.\n",
      "     |      \n",
      "     |      Using this method, you get back a ndarray object, and subsequent operations on the returned ndarray object will be singlenode.\n",
      "     |  \n",
      "     |  __div__(self, other)\n",
      "     |      Performs division (Python 2 way).\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __floordiv__(self, other)\n",
      "     |  \n",
      "     |  __ge__(self, other)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Implements evaluation of right indexing operations such as m[1,1], m[0:1,], m[:, 0:1]\n",
      "     |  \n",
      "     |  __gt__(self, other)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __init__(self, data, op=None)\n",
      "     |      Constructs a lazy matrix\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data: NumPy ndarray, Pandas DataFrame, scipy sparse matrix or PySpark DataFrame. (data cannot be None for external users, 'data=None' is used internally for lazy evaluation).\n",
      "     |  \n",
      "     |  __le__(self, other)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, other)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __matmul__(self, other)\n",
      "     |      Performs matrix multiplication (infix operator: @). See PEP 465)\n",
      "     |  \n",
      "     |  __mod__(self, other)\n",
      "     |  \n",
      "     |  __mul__(self, other)\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __numpy_ufunc__(self, func, method, pos, inputs, **kwargs)\n",
      "     |      This function enables systemml matrix to be compatible with NumPy's ufuncs.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func:  ufunc object that was called.\n",
      "     |      method: string indicating which Ufunc method was called (one of \"__call__\", \"reduce\", \"reduceat\", \"accumulate\", \"outer\", \"inner\").\n",
      "     |      pos: index of self in inputs.\n",
      "     |      inputs:  tuple of the input arguments to the ufunc\n",
      "     |      kwargs: dictionary containing the optional input arguments of the ufunc.\n",
      "     |  \n",
      "     |  __or__(self, other)\n",
      "     |  \n",
      "     |  __pow__(self, other)\n",
      "     |  \n",
      "     |  __radd__(self, other)\n",
      "     |  \n",
      "     |  __rdiv__(self, other)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      This function helps to debug matrix class and also examine the generated PyDML script\n",
      "     |  \n",
      "     |  __rfloordiv__(self, other)\n",
      "     |  \n",
      "     |  __rmod__(self, other)\n",
      "     |  \n",
      "     |  __rmul__(self, other)\n",
      "     |  \n",
      "     |  __rpow__(self, other)\n",
      "     |  \n",
      "     |  __rsub__(self, other)\n",
      "     |  \n",
      "     |  __rtruediv__(self, other)\n",
      "     |      Performs division (Python 3 way).\n",
      "     |  \n",
      "     |  __setitem__(self, index, value)\n",
      "     |      Implements evaluation of left indexing operations such as m[1,1]=2\n",
      "     |  \n",
      "     |  __sub__(self, other)\n",
      "     |  \n",
      "     |  __truediv__(self, other)\n",
      "     |      Performs division (Python 3 way).\n",
      "     |  \n",
      "     |  abs(self)\n",
      "     |  \n",
      "     |  acos(self)\n",
      "     |  \n",
      "     |  arccos(self)\n",
      "     |  \n",
      "     |  arcsin(self)\n",
      "     |  \n",
      "     |  arctan(self)\n",
      "     |  \n",
      "     |  argmax(self, axis=None)\n",
      "     |      Returns the indices of the maximum values along an axis.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      axis : int, optional (only axis=1, i.e. rowIndexMax is supported in this version)\n",
      "     |  \n",
      "     |  argmin(self, axis=None)\n",
      "     |      Returns the indices of the minimum values along an axis.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      axis : int, optional  (only axis=1, i.e. rowIndexMax is supported in this version)\n",
      "     |  \n",
      "     |  asfptype(self)\n",
      "     |  \n",
      "     |  asin(self)\n",
      "     |  \n",
      "     |  astype(self, t)\n",
      "     |  \n",
      "     |  atan(self)\n",
      "     |  \n",
      "     |  ceil(self)\n",
      "     |  \n",
      "     |  ceiling(self)\n",
      "     |  \n",
      "     |  cos(self)\n",
      "     |  \n",
      "     |  cosh(self)\n",
      "     |  \n",
      "     |  cumsum(self, axis=None)\n",
      "     |      Returns the indices of the maximum values along an axis.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      axis : int, optional (only axis=0, i.e. cumsum along the rows is supported in this version)\n",
      "     |  \n",
      "     |  deg2rad(self)\n",
      "     |      Convert angles from degrees to radians.\n",
      "     |  \n",
      "     |  dot(self, other)\n",
      "     |      Numpy way of performing matrix multiplication\n",
      "     |  \n",
      "     |  eval(self)\n",
      "     |      This is a convenience function that calls the global eval method\n",
      "     |  \n",
      "     |  exp(self)\n",
      "     |  \n",
      "     |  exp2(self)\n",
      "     |  \n",
      "     |  expm1(self)\n",
      "     |  \n",
      "     |  floor(self)\n",
      "     |  \n",
      "     |  get_shape(self)\n",
      "     |  \n",
      "     |  hstack(self, other)\n",
      "     |      Stack matrices horizontally (column wise). Invokes cbind internally.\n",
      "     |  \n",
      "     |  ldexp(self, other)\n",
      "     |  \n",
      "     |  log(self, y=None)\n",
      "     |  \n",
      "     |  log10(self)\n",
      "     |  \n",
      "     |  log1p(self)\n",
      "     |  \n",
      "     |  log2(self)\n",
      "     |  \n",
      "     |  logaddexp(self, other)\n",
      "     |  \n",
      "     |  logaddexp2(self, other)\n",
      "     |  \n",
      "     |  logical_not(self)\n",
      "     |  \n",
      "     |  max(self, other=None, axis=None)\n",
      "     |      Compute the maximum value along the specified axis\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other: matrix or numpy array (& other supported types) or scalar\n",
      "     |      axis : int, optional\n",
      "     |  \n",
      "     |  mean(self, axis=None)\n",
      "     |      Compute the arithmetic mean along the specified axis\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      axis : int, optional\n",
      "     |  \n",
      "     |  min(self, other=None, axis=None)\n",
      "     |      Compute the minimum value along the specified axis\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other: matrix or numpy array (& other supported types) or scalar\n",
      "     |      axis : int, optional\n",
      "     |  \n",
      "     |  mod(self, other)\n",
      "     |  \n",
      "     |  moment(self, moment=1, axis=None)\n",
      "     |      Calculates the nth moment about the mean\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      moment : int\n",
      "     |          can be 1, 2, 3 or 4\n",
      "     |      axis : int, optional\n",
      "     |  \n",
      "     |  negative(self)\n",
      "     |  \n",
      "     |  ones_like(self)\n",
      "     |  \n",
      "     |  print_ast(self)\n",
      "     |      Please use m.print_ast() and/or  type `m` for debugging. Here is a sample session:\n",
      "     |      \n",
      "     |      >>> npm = np.ones((3,3))\n",
      "     |      >>> m1 = sml.matrix(npm + 3)\n",
      "     |      >>> m2 = sml.matrix(npm + 5)\n",
      "     |      >>> m3 = m1 + m2\n",
      "     |      >>> m3\n",
      "     |      mVar2 = load(\" \", format=\"csv\")\n",
      "     |      mVar1 = load(\" \", format=\"csv\")\n",
      "     |      mVar3 = mVar1 + mVar2\n",
      "     |      save(mVar3, \" \")\n",
      "     |      >>> m3.print_ast()\n",
      "     |      - [mVar3] (op).\n",
      "     |        - [mVar1] (data).\n",
      "     |        - [mVar2] (data).\n",
      "     |  \n",
      "     |  prod(self)\n",
      "     |      Return the product of all cells in matrix\n",
      "     |  \n",
      "     |  rad2deg(self)\n",
      "     |      Convert angles from radians to degrees.\n",
      "     |  \n",
      "     |  reciprocal(self)\n",
      "     |  \n",
      "     |  remainder(self, other)\n",
      "     |  \n",
      "     |  remove_empty(self, axis=None)\n",
      "     |      Removes all empty rows or columns from the input matrix target X according to specified axis.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      axis : int (0 or 1)\n",
      "     |  \n",
      "     |  replace(self, pattern=None, replacement=None)\n",
      "     |      Removes all empty rows or columns from the input matrix target X according to specified axis.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      pattern : float or int\n",
      "     |      replacement : float or int\n",
      "     |  \n",
      "     |  round(self)\n",
      "     |  \n",
      "     |  save(self, file, format='csv')\n",
      "     |      Allows user to save a matrix to filesystem\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      file: filepath\n",
      "     |      format: can be csv, text or binary or mm\n",
      "     |  \n",
      "     |  sd(self, axis=None)\n",
      "     |      Compute the standard deviation along the specified axis\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      axis : int, optional\n",
      "     |  \n",
      "     |  set_shape(self, shape)\n",
      "     |  \n",
      "     |  sign(self)\n",
      "     |  \n",
      "     |  sin(self)\n",
      "     |  \n",
      "     |  sinh(self)\n",
      "     |  \n",
      "     |  sqrt(self)\n",
      "     |  \n",
      "     |  square(self)\n",
      "     |  \n",
      "     |  sum(self, axis=None)\n",
      "     |      Compute the sum along the specified axis\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      axis : int, optional\n",
      "     |  \n",
      "     |  tan(self)\n",
      "     |  \n",
      "     |  tanh(self)\n",
      "     |  \n",
      "     |  toDF(self)\n",
      "     |      This is a convenience function that calls the global eval method and then converts the matrix object into DataFrame.\n",
      "     |  \n",
      "     |  toNumPy(self)\n",
      "     |      This is a convenience function that calls the global eval method and then converts the matrix object into NumPy array.\n",
      "     |  \n",
      "     |  toPandas(self)\n",
      "     |      This is a convenience function that calls the global eval method and then converts the matrix object into Pandas DataFrame.\n",
      "     |  \n",
      "     |  trace(self)\n",
      "     |      Return the sum of the cells of the main diagonal square matrix\n",
      "     |  \n",
      "     |  transpose(self)\n",
      "     |      Transposes the matrix.\n",
      "     |  \n",
      "     |  var(self, axis=None)\n",
      "     |      Compute the variance along the specified axis.\n",
      "     |      We assume that delta degree of freedom is 1 (unlike NumPy which assumes ddof=0).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      axis : int, optional\n",
      "     |  \n",
      "     |  vstack(self, other)\n",
      "     |      Stack matrices vertically (row wise). Invokes rbind internally.\n",
      "     |  \n",
      "     |  zeros_like(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  shape\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  THROW_ARRAY_CONVERSION_ERROR = False\n",
      "     |  \n",
      "     |  __array_priority__ = 10.2\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  dml = []\n",
      "     |  \n",
      "     |  ml = None\n",
      "     |  \n",
      "     |  ndim = 2\n",
      "     |  \n",
      "     |  script = None\n",
      "     |  \n",
      "     |  systemmlVarID = 0\n",
      "     |  \n",
      "     |  visited = []\n",
      "\n",
      "FUNCTIONS\n",
      "    _java2py(sc, obj)\n",
      "        Convert Java object to Python.\n",
      "    \n",
      "    convertImageToNumPyArr(im, img_shape=None, add_rotated_images=False, add_mirrored_images=False, color_mode='RGB', mean=None)\n",
      "        # Example usage: convertImageToNumPyArr(im, img_shape=(3, 224, 224), add_rotated_images=True, add_mirrored_images=True)\n",
      "        # The above call returns a numpy array of shape (6, 50176) in NCHW format\n",
      "    \n",
      "    convertToLabeledDF(sparkSession, X, y=None)\n",
      "    \n",
      "    convertToMatrixBlock(sc, src, maxSizeBlockInMB=8)\n",
      "    \n",
      "    convertToNumPyArr(sc, mb)\n",
      "    \n",
      "    convertToPandasDF(X)\n",
      "    \n",
      "    convert_caffemodel(sc, deploy_file, caffemodel_file, output_dir, format='binary', is_caffe_installed=False)\n",
      "        Saves the weights and bias in the caffemodel file to output_dir in the specified format.\n",
      "        This method does not requires caffe to be installed.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sc: SparkContext\n",
      "            SparkContext\n",
      "        \n",
      "        deploy_file: string\n",
      "            Path to the input network file\n",
      "        \n",
      "        caffemodel_file: string\n",
      "            Path to the input caffemodel file\n",
      "        \n",
      "        output_dir: string\n",
      "            Path to the output directory\n",
      "        \n",
      "        format: string\n",
      "            Format of the weights and bias (can be binary, csv or text)\n",
      "        \n",
      "        is_caffe_installed: bool\n",
      "            True if caffe is installed\n",
      "    \n",
      "    convert_lmdb_to_jpeg(lmdb_img_file, output_dir)\n",
      "        Saves the images in the lmdb file as jpeg in the output_dir. This method requires caffe to be installed along with lmdb and cv2 package.\n",
      "        To install cv2 package, do `pip install opencv-python`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        lmdb_img_file: string\n",
      "            Path to the input lmdb file\n",
      "        \n",
      "        output_dir: string\n",
      "            Output directory for images (local filesystem)\n",
      "    \n",
      "    debug_array_conversion(throwError)\n",
      "    \n",
      "    dml(scriptString)\n",
      "        Create a dml script object based on a string.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        scriptString: string\n",
      "            Can be a path to a dml script or a dml script itself.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        script: Script instance\n",
      "            Instance of a script object.\n",
      "    \n",
      "    dmlFromFile(filePath)\n",
      "        Create a dml script object based on a file path.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        filePath: string\n",
      "            Path to a dml script.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        script: Script instance\n",
      "            Instance of a script object.\n",
      "    \n",
      "    dmlFromResource(resourcePath)\n",
      "        Create a dml script object based on a resource path.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        resourcePath: string\n",
      "            Path to a dml script on the classpath.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        script: Script instance\n",
      "            Instance of a script object.\n",
      "    \n",
      "    dmlFromUrl(url)\n",
      "        Create a dml script object based on a url.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        url: string\n",
      "            URL to a dml script.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        script: Script instance\n",
      "            Instance of a script object.\n",
      "    \n",
      "    eval(outputs, execute=True)\n",
      "        Executes the unevaluated DML script and computes the matrices specified by outputs.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        outputs: list of matrices or a matrix object\n",
      "        execute: specified whether to execute the unevaluated operation or just return the script.\n",
      "    \n",
      "    full(shape, fill_value)\n",
      "        Return a new array of given shape filled with fill_value.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape: tuple of length 2\n",
      "        fill_value: float or int\n",
      "    \n",
      "    getDatasetMean(dataset_name)\n",
      "        Parameters\n",
      "        ----------\n",
      "        dataset_name: Name of the dataset used to train model. This name is artificial name based on dataset used to train the model.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mean: Mean value of model if its defined in the list DATASET_MEAN else None.\n",
      "    \n",
      "    getHopDAG(ml, script, lines=None, conf=None, apply_rewrites=True, with_subgraph=False)\n",
      "        Compile a DML / PyDML script.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        ml: MLContext instance\n",
      "            MLContext instance.\n",
      "            \n",
      "        script: Script instance\n",
      "            Script instance defined with the appropriate input and output variables.\n",
      "        \n",
      "        lines: list of integers\n",
      "            Optional: only display the hops that have begin and end line number equals to the given integers.\n",
      "        \n",
      "        conf: SparkConf instance\n",
      "            Optional spark configuration\n",
      "            \n",
      "        apply_rewrites: boolean\n",
      "            If True, perform static rewrites, perform intra-/inter-procedural analysis to propagate size information into functions and apply dynamic rewrites\n",
      "        \n",
      "        with_subgraph: boolean\n",
      "            If False, the dot graph will be created without subgraphs for statement blocks. \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        hopDAG: string\n",
      "            hop DAG in dot format\n",
      "    \n",
      "    getNumCols(numPyArr)\n",
      "    \n",
      "    load(file, format='csv')\n",
      "        Allows user to load a matrix from filesystem\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        file: filepath\n",
      "        format: can be csv, text or binary or mm\n",
      "    \n",
      "    pydml(scriptString)\n",
      "        Create a pydml script object based on a string.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        scriptString: string\n",
      "            Can be a path to a pydml script or a pydml script itself.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        script: Script instance\n",
      "            Instance of a script object.\n",
      "    \n",
      "    pydmlFromFile(filePath)\n",
      "        Create a pydml script object based on a file path.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        filePath: string\n",
      "            Path to a pydml script.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        script: Script instance\n",
      "            Instance of a script object.\n",
      "    \n",
      "    pydmlFromResource(resourcePath)\n",
      "        Create a pydml script object based on a resource path.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        resourcePath: string\n",
      "            Path to a pydml script on the classpath.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        script: Script instance\n",
      "            Instance of a script object.\n",
      "    \n",
      "    pydmlFromUrl(url)\n",
      "        Create a pydml script object based on a url.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        url: string\n",
      "            URL to a pydml script.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        script: Script instance\n",
      "            Instance of a script object.\n",
      "    \n",
      "    seq(start=None, stop=None, step=1)\n",
      "        Creates a single column vector with values starting from <start>, to <stop>, in increments of <step>.\n",
      "        Note: Unlike Numpy's arange which returns a row-vector, this returns a column vector.\n",
      "        Also, Unlike Numpy's arange which doesnot include stop, this method includes stop in the interval.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start: int or float [Optional: default = 0]\n",
      "        stop: int or float\n",
      "        step : int float [Optional: default = 1]\n",
      "    \n",
      "    setSparkContext(sc)\n",
      "        Before using the matrix, the user needs to invoke this function if SparkContext is not previously created in the session.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sc: SparkContext\n",
      "            SparkContext\n",
      "    \n",
      "    set_lazy(isLazy)\n",
      "        This method allows users to set whether the matrix operations should be executed in lazy manner.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        isLazy: True if matrix operations should be evaluated in lazy manner.\n",
      "    \n",
      "    solve(A, b)\n",
      "        Computes the least squares solution for system of linear equations A %*% x = b\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import datasets\n",
      "        >>> import SystemML as sml\n",
      "        >>> from pyspark.sql import SparkSession\n",
      "        >>> diabetes = datasets.load_diabetes()\n",
      "        >>> diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "        >>> X_train = diabetes_X[:-20]\n",
      "        >>> X_test = diabetes_X[-20:]\n",
      "        >>> y_train = diabetes.target[:-20]\n",
      "        >>> y_test = diabetes.target[-20:]\n",
      "        >>> sml.setSparkContext(sc)\n",
      "        >>> X = sml.matrix(X_train)\n",
      "        >>> y = sml.matrix(y_train)\n",
      "        >>> A = X.transpose().dot(X)\n",
      "        >>> b = X.transpose().dot(y)\n",
      "        >>> beta = sml.solve(A, b).toNumPy()\n",
      "        >>> y_predicted = X_test.dot(beta)\n",
      "        >>> print('Residual sum of squares: %.2f' % np.mean((y_predicted - y_test) ** 2))\n",
      "        Residual sum of squares: 25282.12\n",
      "\n",
      "DATA\n",
      "    SUPPORTED_TYPES = (<class 'numpy.ndarray'>, <class 'pandas.core.frame....\n",
      "    __all__ = ['MLResults', 'MLContext', 'Script', 'Matrix', 'dml', 'pydml...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\user\\appdata\\roaming\\python\\python35\\site-packages\\systemml\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(systemml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+--------------+-------+\n",
      "| id|hour|mobile|  userFeatures|clicked|\n",
      "+---+----+------+--------------+-------+\n",
      "|  0|  18|   1.0|[0.0,10.0,0.5]|    1.0|\n",
      "+---+----+------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.createDataFrame(\n",
    "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n",
    "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
